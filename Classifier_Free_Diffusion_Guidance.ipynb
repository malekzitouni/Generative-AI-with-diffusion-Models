{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXFoK05GFAPR+HdIv1Xreo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malekzitouni/Generative-AI-with-diffusion-Models/blob/main/Classifier_Free_Diffusion_Guidance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have been able to train a model to generate images of clothing using the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. However, there is no way for the user to specify what kind of images should be generated. Let's fix that by using [Classifier-Free Diffusion Guidance](https://arxiv.org/pdf/2207.12598.pdf), a relatively simple way to create a [Conditional Diffusion Model](https://github.com/TeaPearce/Conditional_Diffusion_MNIST/tree/main).\n"
      ],
      "metadata": {
        "id": "jss-YSU-pYMw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DujpVIKpPa-"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Visualization tools\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "# User defined libraries\n",
        "from utils import other_utils\n",
        "from utils import ddpm_utils\n",
        "from utils import UNet_utils\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 16\n",
        "IMG_CH = 1\n",
        "BATCH_SIZE = 128\n",
        "N_CLASSES = 10\n",
        "data, dataloader = other_utils.load_transformed_fashionMNIST(IMG_SIZE, BATCH_SIZE)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgrD1iRUqntQ",
        "outputId": "5f1ffa2b-d0ad-4408-df36-d3b719f763b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:04<00:00, 6.16MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 131kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.48MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.69MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nrows = 10\n",
        "ncols = 15\n",
        "\n",
        "T = nrows * ncols\n",
        "B_start = 0.0001\n",
        "B_end = 0.02\n",
        "B = torch.linspace(B_start, B_end, T).to(device)\n",
        "ddpm = ddpm_utils.DDPM(B, device)"
      ],
      "metadata": {
        "id": "QsnSdG9bqsE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, our `UNet` is slightly different. We've added a few changes and moved the U-Net architecture into its own [UNet_utils.py](utils/UNet_utils.py) folder.\n",
        "\n",
        "In the `__init__` function, we've added a new parameter: `c_embed_dim`. Like for timestep `t`, we can create an embedding for our class categories.\n",
        "\n",
        "```python\n",
        "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_embed_dim)\n",
        "        self.t_emb1 = EmbedBlock(t_embed_dim, up_chs[0])\n",
        "        self.t_emb2 = EmbedBlock(t_embed_dim, up_chs[1])\n",
        "        self.c_embed1 = EmbedBlock(c_embed_dim, up_chs[0])  # New\n",
        "        self.c_embed2 = EmbedBlock(c_embed_dim, up_chs[1])  # New\n",
        "```\n",
        "\n",
        "Next, in the `forward` function, we have two new parameters: `c` and `c_mask`.\n",
        "* `c` is a vector representing our categorical input. It can be a [one-hot encoding](https://www.kaggle.com/code/dansbecker/using-categorical-data-with-one-hot-encoding) or an embedding vector.\n",
        "* `c_mask` is used to randomly set values within `c` to zero. This helps the model learn what an average output might be without a categorical input, like in the previous notebook.\n",
        "\n",
        "```python\n",
        "        c = c * c_mask\n",
        "        c_emb1 = self.c_embed1(c)\n",
        "        c_emb2 = self.c_embed2(c)\n",
        "```\n",
        "\n",
        "There are many different ways we can combine this embedded categorical information into the model. One popular method is with [scaling and shifting](https://arxiv.org/pdf/2210.08823.pdf). We can scale (multiply) our categorical embedding to the latent image and then (add) our time embedding `t_emb`. The scale and shift act as a sort of variance and average respectively.\n",
        "\n",
        "```python\n",
        "        up0 = self.up0(latent_vec)\n",
        "        up1 = self.up1(c_emb1 * up0 + t_emb1, down2)  # Changed\n",
        "        up2 = self.up2(c_emb2 * up1 + t_emb2, down1)  # Changed\n",
        "        return self.out(torch.cat((up2, down0), 1))\n",
        "```"
      ],
      "metadata": {
        "id": "FwCePHIFrF7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will convert our label to a format that can be processed by the model using the `get_context_mask` function below. Since our label is a single integer, we can use [F.one_hot](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html) to turn it into an encoding vector.\n",
        "\n",
        "To randomly set values from this one hot encoding to zero, we can use the [Bernoulli](https://mathworld.wolfram.com/BernoulliDistribution.html) distribution. This distribution is like flipping a weighted coin. \"Heads\" will land $p$ percent of the time and \"Tails\" will land $1-p$ percent of the time. In this case, our `drop_prob` represents \"Tails\".\n",
        "\n",
        "<center><img src=\"/content/images/bernoulli.png\"/></center>"
      ],
      "metadata": {
        "id": "JvxYpldCrbev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context_mask(c, drop_prob):\n",
        "    c_hot = F.one_hot(c.to(torch.int64), num_classes=N_CLASSES).to(device)\n",
        "    c_mask = torch.bernoulli(torch.ones_like(c_hot).float() - drop_prob).to(device)\n",
        "    return c_hot, c_mask"
      ],
      "metadata": {
        "id": "rSvpoMvRrEsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet_utils.UNet(\n",
        "    T, IMG_CH, IMG_SIZE, down_chs=(64, 64, 128), t_embed_dim=8, c_embed_dim=N_CLASSES\n",
        ")\n",
        "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
        "model = torch.compile(model.to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht7bDTRHrIUT",
        "outputId": "e474a30c-f391-41b3-bfe7-fa7dd979f405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num params:  2002561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\n",
        "    \"Top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]"
      ],
      "metadata": {
        "id": "YRiJJw9A2kR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "epochs = 3\n",
        "preview_c = 0\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        c_drop_prob = 0.1\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).float()\n",
        "        x = batch[0].to(device)\n",
        "        c_hot, c_mask = get_context_mask(batch[1], c_drop_prob)  # New\n",
        "        loss = ddpm.get_loss(model, x, t, c_hot, c_mask)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 1 == 0 and step % 100 == 0:\n",
        "            class_name = class_names[preview_c]\n",
        "            print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()} | C: {class_name}\")\n",
        "            c_drop_prob = 0 # Do not drop context for preview\n",
        "            c_hot, c_mask = get_context_mask(torch.Tensor([preview_c]), c_drop_prob)\n",
        "            ddpm.sample_images(model, IMG_CH, IMG_SIZE, ncols, c_hot, c_mask)\n",
        "            preview_c = (preview_c + 1) % N_CLASSES"
      ],
      "metadata": {
        "id": "KSYuAkrZ7Zxn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}