{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbCcCPJ0747IAic5ze1Uxz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malekzitouni/Generative-AI-with-diffusion-Models/blob/main/Optimizations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchview"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7g72QetW9lc",
        "outputId": "7f52cb24-c55c-4572-d130-4206ace0da65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchview\n",
            "  Downloading torchview-0.2.6-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: torchview\n",
            "Successfully installed torchview-0.2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x5-Swx5WcdB",
        "outputId": "f33cc93d-4bc2-4534-b4fb-49d0caad1567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 273kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.03MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.97MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Visualization tools\n",
        "import matplotlib.pyplot as plt\n",
        "from torchview import draw_graph\n",
        "import graphviz\n",
        "from IPython.display import Image\n",
        "\n",
        "# User defined libraries\n",
        "from utils import other_utils\n",
        "from utils import ddpm_utils\n",
        "\n",
        "IMG_SIZE = 16\n",
        "IMG_CH = 1\n",
        "BATCH_SIZE = 128\n",
        "data, dataloader = other_utils.load_transformed_fashionMNIST(IMG_SIZE, BATCH_SIZE)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nrows = 10\n",
        "ncols = 15\n",
        "\n",
        "T = nrows * ncols\n",
        "B_start = 0.0001\n",
        "B_end = 0.02\n",
        "B = torch.linspace(B_start, B_end, T).to(device)\n",
        "ddpm = ddpm_utils.DDPM(B, device)\n",
        "ddpm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsT3hLi9ab14",
        "outputId": "97b7fd1c-faa4-4733-8ae8-cae26b353356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<utils.ddpm_utils.DDPM at 0x7effb5a4f4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[ReLU](https://www.kaggle.com/code/dansbecker/rectified-linear-units-relu-in-deep-learning) is a popular choice for an activation function because it is computationally quick and easy to calculate the gradient for. Unfortunately, it isn't perfect. When the bias term becomes largely negative, a ReLU neuron [\"dies\"](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks) because both its output and gradient are zero.\n",
        "\n",
        "At a slight cost in computational power, [GELU](https://arxiv.org/pdf/1606.08415.pdf) seeks to rectify the rectified linear unit by mimicking the shape of the ReLU function while avoiding a zero gradient.\n",
        "\n",
        "In this small example with FashionMNIST, it is unlikely we will see any dead neurons. However, the larger a model gets, the more likely it can face the dying ReLU phenomenon."
      ],
      "metadata": {
        "id": "mAZi7Wo9a7rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.Conv2d(in_ch, out_ch, 3, 1, 1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU()\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "Y3p99k-Wa8Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous notebook, we used [Max Pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) to halve the size of our latent image, but is that the best technique? There are [many types of pooling layers](https://pytorch.org/docs/stable/nn.html#pooling-layers) including Min Pooling and Average Pooling. How about we let the neural network decide what is important.\n",
        "\n",
        "Enter the [einops](https://einops.rocks/1-einops-basics/) library and the [Rearrange](https://einops.rocks/api/rearrange/) layer. We can assign each layer a variable and use that to rearrange our values. Additionally, we can use parentheses `()` to identify a set of variables that are multiplied together.\n",
        "\n",
        "For example, in the code block below, we have:\n",
        "\n",
        "`Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2)`\n",
        "\n",
        "* `b` is our batch dimension\n",
        "* `c` is our channel dimension\n",
        "* `h` is our height dimension\n",
        "* `w` is our width dimension\n",
        "\n",
        "We also have a `p1` and `p2` value that are both equal to `2`. The left portion of the equation before the arrow is saying \"split the height and width dimensions in half. The right portion of the equation after the arrow is saying \"stack the split dimensions along the channel dimension\".\n",
        "\n",
        "The code block below sets up a `test_image` to practice on. Try swapping `h` with `p1` on the left side of the arrow. What happens? How about when `w` and `p2` are swapped? What happens when `p1` is set to `3` instead of `2`?"
      ],
      "metadata": {
        "id": "GDYSL-9YbRdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "rearrange = Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2)\n",
        "\n",
        "test_image = [\n",
        "    [\n",
        "        [\n",
        "            [1, 2, 3, 4, 5, 6],\n",
        "            [7, 8, 9, 10, 11, 12],\n",
        "            [13, 14, 15, 16, 17, 18],\n",
        "            [19, 20, 21, 22, 23, 24],\n",
        "            [25, 26, 27, 28, 29, 30],\n",
        "            [31, 32, 33, 34, 35, 36],\n",
        "        ]\n",
        "    ]\n",
        "]\n",
        "test_image = torch.tensor(test_image)\n",
        "print(test_image)\n",
        "output = rearrange(test_image)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laakIUahbR8R",
        "outputId": "140a2761-6226-4564-846a-e5098b5a261e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 1,  2,  3,  4,  5,  6],\n",
            "          [ 7,  8,  9, 10, 11, 12],\n",
            "          [13, 14, 15, 16, 17, 18],\n",
            "          [19, 20, 21, 22, 23, 24],\n",
            "          [25, 26, 27, 28, 29, 30],\n",
            "          [31, 32, 33, 34, 35, 36]]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 1,  3,  5],\n",
              "          [13, 15, 17],\n",
              "          [25, 27, 29]],\n",
              "\n",
              "         [[ 2,  4,  6],\n",
              "          [14, 16, 18],\n",
              "          [26, 28, 30]],\n",
              "\n",
              "         [[ 7,  9, 11],\n",
              "          [19, 21, 23],\n",
              "          [31, 33, 35]],\n",
              "\n",
              "         [[ 8, 10, 12],\n",
              "          [20, 22, 24],\n",
              "          [32, 34, 36]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.rearrange = Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2)\n",
        "        self.conv = GELUConvBlock(4 * in_chs, in_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "OdCdG_9mcFWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super(DownBlock, self).__init__()\n",
        "        layers = [\n",
        "            GELUConvBlock(in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            RearrangePoolBlock(out_chs, group_size)\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "gZHhaYh0cZ0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super(UpBlock, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(2 * in_chs, out_chs, 2, 2),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = torch.cat((x, skip), 1)\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6WO9-tCrcbZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The better the model understands the timestep it is in for the reverse diffusion process, the better it will be able to correctly identify the added noise. In the previous notebook, we created an embedding for `t/T`. Can we help the model interpret this better?\n",
        "\n",
        "Before diffusion models, this was a problem that plagued natural language processing. For long dialogues, how can we capture where we are? The goal was to find a way to uniquely represent a large range of discrete numbers with a small number of continuous numbers. Using a single float is ineffective since the neural network will interpret timesteps as continuous rather than discrete. [Researchers](https://arxiv.org/pdf/1706.03762.pdf) ultimately settled on a sum of sines and cosines.\n",
        "\n",
        "For an excellent explanation for why this works and how this technique was likely developed, please refer to Jonathan Kernes' [Master Positional Encoding](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)."
      ],
      "metadata": {
        "id": "Quvq9raYglgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class SinusoidalPositionEmbedBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "Qz8IX4HBchTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedBlock(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedBlock, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Unflatten(1, (emb_dim, 1, 1))\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "oup9HeH7g7ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = GELUConvBlock(in_chs, out_chs, group_size)\n",
        "        self.conv2 = GELUConvBlock(out_chs, out_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(x1)\n",
        "        out = x1 + x2\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ax14bWH_hN3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        img_chs = IMG_CH\n",
        "        down_chs = (64, 64, 128)\n",
        "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
        "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
        "        t_dim = 8\n",
        "        group_size_base = 4\n",
        "        small_group_size = 2 * group_size_base # New\n",
        "        big_group_size = 8 * group_size_base  # New\n",
        "\n",
        "\n",
        "        # Inital convolution\n",
        "        self.down0 = ResidualConvBlock(img_chs, down_chs[0], small_group_size) # New\n",
        "\n",
        "        # Downsample\n",
        "        self.down1 = DownBlock(down_chs[0], down_chs[1], big_group_size) # New\n",
        "        self.down2 = DownBlock(down_chs[1], down_chs[2], big_group_size) # New\n",
        "        self.to_vec = nn.Sequential(nn.Flatten(), nn.GELU())\n",
        "\n",
        "        # Embeddings\n",
        "        self.dense_emb = nn.Sequential(\n",
        "            nn.Linear(down_chs[2]*latent_image_size**2, down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[2]*latent_image_size**2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_dim) # New\n",
        "        self.temb_1 = EmbedBlock(t_dim, up_chs[0])\n",
        "        self.temb_2 = EmbedBlock(t_dim, up_chs[1])\n",
        "\n",
        "        # Upsample\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
        "            GELUConvBlock(up_chs[0], up_chs[0], big_group_size) # New\n",
        "        )\n",
        "        self.up1 = UpBlock(up_chs[0], up_chs[1], big_group_size) # New\n",
        "        self.up2 = UpBlock(up_chs[1], up_chs[2], big_group_size) # New\n",
        "\n",
        "        # Match output channels and one last concatenation\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * up_chs[-1], up_chs[-1], 3, 1, 1),\n",
        "            nn.GroupNorm(small_group_size, up_chs[-1]), # New\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(up_chs[-1], img_chs, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        down0 = self.down0(x)\n",
        "        down1 = self.down1(down0)\n",
        "        down2 = self.down2(down1)\n",
        "        latent_vec = self.to_vec(down2)\n",
        "\n",
        "        latent_vec = self.dense_emb(latent_vec)\n",
        "        t = t.float() / T  # Convert from [0, T] to [0, 1]\n",
        "        t = self.sinusoidaltime(t) # New\n",
        "        temb_1 = self.temb_1(t)\n",
        "        temb_2 = self.temb_2(t)\n",
        "\n",
        "        up0 = self.up0(latent_vec)\n",
        "        up1 = self.up1(up0+temb_1, down2)\n",
        "        up2 = self.up2(up1+temb_2, down1)\n",
        "        return self.out(torch.cat((up2, down0), 1)) # New"
      ],
      "metadata": {
        "id": "NLV-mWuNhU-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet()\n",
        "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
        "model = torch.compile(model.to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UDBLL9EiW8-",
        "outputId": "e39e3b71-67ad-453b-d49b-70daecb8c3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num params:  1979777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "epochs = 5\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).float()\n",
        "        x = batch[0].to(device)\n",
        "        loss = ddpm.get_loss(model, x, t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 1 == 0 and step % 100 == 0:\n",
        "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
        "            ddpm.sample_images(model, IMG_CH, IMG_SIZE, ncols)"
      ],
      "metadata": {
        "id": "l2t9C2gKMYm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "plt.figure(figsize=(8,8))\n",
        "ncols = 3 # Should evenly divide T\n",
        "for _ in range(10):\n",
        "    ddpm.sample_images(model, IMG_CH, IMG_SIZE, ncols)"
      ],
      "metadata": {
        "id": "iVtyK0tvjTtD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}